{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>",
   "id": "a1c16b3f5cb92e49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predicting Demand for Washington D.C.'s Bike Share System",
   "id": "255526c626fe2a43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Background\n",
    "You have just been hired as the first data scientist working for Capital Bikeshare, the organization which runs the Washington D.C. bike sharing system. The first major project they have asked you to work on is to build a model to predict demand for the shared bikes in the system for each hour of each day.  \n",
    "\n",
    "Having an accurate understanding of the expected demand is critical to the successful operation of Capital Bikeshare.  If they underestimate demand and have too few bikes available, potential users of the system are not able to find a bike to use and so get upset and are less likely to use the system in the future.  If they overestimate by too much, they end up with too many bikes sitting around not being used.  In the real-world, one of the things that makes this challenging is that they have to predict demand **for each pick-up hub location**.  To keep things simple for our final, we will focus on predicting aggregate demand.\n",
    "\n",
    "Our task in this exercise is to build the pipeline to convert raw data into features to use in a ML model. The model itself that you will use has already been set up for you (a linear regression model which has been put into a separate script you will import) and **you cannot change the model**, only the data pipeline.\n",
    "\n",
    "## Data\n",
    "You have been given two csv files of data to use in your analysis.  The first file (\"2011-2012_bikes.csv\") contains historical demand data from the past two years of operation. The dataset contains the following columns:\n",
    "- dteday : date \n",
    "- hr : hour (0 to 23) \n",
    "- cnt: count of total rental bikes \n",
    "\n",
    "The second file (\"2011-2012_weather.csv\") contains weather information for the same time period.  This dataset contains the following columns:  \n",
    "- dteday : date \n",
    "- hr : hour (0 to 23) \n",
    "- weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp : Temperature in Celsius\n",
    "- atemp: Feels-like temperature in Celsius\n",
    "- hum: Humidity\n",
    "- windspeed: Wind speed\n",
    "\n",
    "You may use some or all of the data provided, not all of it is necessarily useful.\n",
    "\n",
    "## Approach\n",
    "Your task in this exercise is to build the pipeline from raw data to features ready for modeling.  There are many possible approaches to doing this, some are better, some are worse.  \n"
   ],
   "id": "ac94d40f320282ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run this before any other code cell\n",
    "# This downloads the csv data files into the same directory where you have saved this notebook\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import os\n",
    "path = Path()\n",
    "\n",
    "# Dictionary of file names and download links\n",
    "files = {'2011-2012_bikes.csv':'https://storage.googleapis.com/aipi_datasets/2011-2012_bikes.csv',\n",
    "        '2011-2012_weather_messy.csv': 'https://storage.googleapis.com/aipi_datasets/2011-2012_weather_messy.csv'}\n",
    "\n",
    "# Download each file\n",
    "for key,value in files.items():\n",
    "    filename = path/key\n",
    "    url = value\n",
    "    # If the file does not already exist in the directory, download it\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url,filename)"
   ],
   "id": "b591a2b76029a7b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "def run_model(X_train,y_train,X_test,y_test):\n",
    "\n",
    "    lin_model = LinearRegression()\n",
    "    lin_model.fit(X_train,y_train)\n",
    "    y_pred = lin_model.predict(X_test)\n",
    "    test_mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    return test_mse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ],
   "id": "717dee8135e0979b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load data\n",
    "Create and run a function `load_data()` to do your data loading and any merging needed.  You can specify the arguments and returns as needed."
   ],
   "id": "48244388dcc43b53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data(file1, file2):\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    merged = pd.merge(df1, df2, how=\"outer\", on=['dteday','hr'])\n",
    "    return merged"
   ],
   "id": "d8a3e17ce5f047ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Clean data\n",
    "Use the cell below to create and run a function `clean_data()` which cleans up the data as needed.  Things you may want to consider at this stage include:  \n",
    "- Checking for and handling any missing values \n",
    "- Identifying any erroneous data and handling \n",
    "- Identifying outliers and determining whether to remove/adjust them or leave them as-is\n",
    "- Etc."
   ],
   "id": "925b986101f20c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_data(s1):\n",
    "    # Standardize dates\n",
    "    s1['dteday'] = pd.to_datetime(s1['dteday'], errors='coerce')\n",
    "\n",
    "    # Convert to epoch seconds\n",
    "    s1['dteday'] = (s1['dteday'] + pd.to_timedelta(s1['hr'], unit='h')).astype('int64') // 10**9\n",
    "\n",
    "    # Forcing Data into numeric if not already\n",
    "    for col in s1.columns[1:]:\n",
    "        s1[col] = pd.to_numeric(s1[col], errors='coerce')\n",
    "\n",
    "    # Dropping non-numerics\n",
    "    s2 = s1.dropna()\n",
    "\n",
    "    print(\"Dropped \" + str(len(s1)-len(s2)) + \" rows from our dataset due to missing values.\")\n",
    "\n",
    "    # Remove extrema\n",
    "    numeric_cols = s2.iloc[:, 2:]\n",
    "    s3 = s2.copy()\n",
    "\n",
    "    for col in numeric_cols.columns:\n",
    "        # Get the Series\n",
    "        series = s2[col]\n",
    "\n",
    "        # Calculate mean and std\n",
    "        col_mn = series.mean()\n",
    "        col_st = series.std()\n",
    "\n",
    "        # Mask rows within 2 std\n",
    "        keep_col = (series > col_mn - 2 * col_st) & (series < col_mn + 2 * col_st)\n",
    "\n",
    "        # Drop rows outside this range\n",
    "        s3 = s3[keep_col]\n",
    "    print(\"Dropped \" +str(len(s2)-len(s3))+ \" columns due to extrema\")\n",
    "\n",
    "    s4 = s3.drop(columns=['hr'])\n",
    "\n",
    "    return s4"
   ],
   "id": "1fcab509a29676c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Split data for training and testing\n",
    "Create and run the function `split_data()` in the cell below to split the data into training and test sets.  You should use all data up to and including July 31 2012 as the training set, and the data for the period August 1 2012 - December 31 2012 as the test set."
   ],
   "id": "f212c1c0a666f0ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_data(s1):\n",
    "    # Define cutoff\n",
    "    cutoff = int(pd.to_datetime(\"2012-08-01 00:00:00\").timestamp())\n",
    "\n",
    "    before = s1[s1['dteday'] <= cutoff]\n",
    "    after  = s1[s1['dteday'] > cutoff]\n",
    "\n",
    "    target = 'cnt'\n",
    "\n",
    "    # Split into X and y\n",
    "    X_train = before.drop(columns=[target])\n",
    "    y_train = before[target]\n",
    "\n",
    "    X_test = after.drop(columns=[target])\n",
    "    y_test = after[target]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ],
   "id": "698f175e6fa9a953"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering\n",
    "Create and run the function `build_features()` below to create any additional derivative features (e.g. time series features) that you wish to use in modeling.  You will need to apply this function to both your training and test sets."
   ],
   "id": "f0fd36481492ea47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "878e02e3b721514c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Selection\n",
    "Use the cell below to create and run the function `feature_select()` which performs feature selection using univariate (filter) methods.  After you analyze the correlations, determine whether you would like to remove any features and do so."
   ],
   "id": "dabb411cb503045f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2015703e48c8775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prepare Features for Modeling\n",
    "Our final step in the pipeline is to prepare our feature set for modeling.  In particular, in this step we need to ensure that any categorical variables we may be using are encoded as numeric values in order for the model to function properly.  You might also consider scaling some of your data.\n",
    "\n",
    "In the below cell create and run a function `prepare_train_feats()` which prepares the training features."
   ],
   "id": "10278a3c89ec223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "364e94a1c82821b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also need to prepare the features in our test set in the same way to feed into the model.  Use the cell below for the function `prepare_test_feats()` which prepares your test set features.",
   "id": "fda34644626484a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "969d23e9cdc63d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Run pipeline\n",
    "Finally, let's bring everything together in a function to run the entire pipeline for our training data.  Complete the function `run_pipeline()` in the cell below.  The function should call any/all of the functions you have defined above which are needed to load the data, transform it and prepare the features for both the training set and the test set."
   ],
   "id": "8d53b9a1f62ed536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_pipeline(bike_filename, weather_filename):\n",
    "    '''\n",
    "    Runs your pipeline (calling the above functions as needed) to transform the raw data into the training and test data sets for modeling\n",
    "\n",
    "    Inputs:\n",
    "        bike_filename(str): name of the file containing the bike data\n",
    "        weather_filename(str): name of the file containing the weather data\n",
    "\n",
    "    Returns:\n",
    "        X_train(pd.DataFrame): dataframe containing the training set inputs\n",
    "        y_train(pd.DataFrame): dataframe containing the training set labels\n",
    "        X_test(pd.DataFrame): dataframe containing the test set inputs\n",
    "        y_test(pd.DataFrame): dataframe containing the test set labels\n",
    "    '''\n",
    "    merged = load_data(bike_filename,weather_filename)\n",
    "    clean = clean_data(merged)\n",
    "    X_train, y_train, X_test, y_test = split_data(clean)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "    "
   ],
   "id": "d951ef9de84b3b94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we've prepared our features we are ready to run our model.  Run the cell below, which trains the model on the training set and calculates and reports the mean squared error (MSE) on the test set.  If everything went well you should have a MSE below 18500",
   "id": "d561fc66deeab19e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bike_datafile = \"2011-2012_bikes.csv\"\n",
    "weather_datafile = \"2011-2012_weather_messy.csv\"\n",
    "X_train, y_train, X_test, y_test = run_pipeline(bike_datafile, weather_datafile)\n",
    "mse_score = run_model(X_train, y_train, X_test, y_test)\n",
    "print('Mean Squared Error on the test set: {:.2f}'.format(mse_score))\n",
    "\n",
    "assert mse_score < 18500"
   ],
   "id": "458868956a32bc94"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
