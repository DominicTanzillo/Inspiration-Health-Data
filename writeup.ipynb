{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22d0f94f6aa32bda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7f4291b2367dcbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Visualization of Inspiration Astronaut Biomarkers After 3 Days in Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72ed6de038e2c607",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Background\n",
    "We are acting as the first data scientist on the Inspiration-Health project. Our first major task is to build a reproducible pipeline to clean, transform, and visualize astronaut biomarker data from the SpaceX Inspiration4 mission.\n",
    "Instead of building predictive models, our focus is on identifying and visualizing significant deviations from baseline values to help communicate health changes during and after spaceflight.\n",
    "\n",
    "\n",
    "## Data\n",
    "We are working with astronaut biospecimen data from the SpaceX Inspiration4 Mission (NASA OSD-575).\n",
    "The cleaned files in `cleaned_data/` include the Comprehensive Metabolic Panel (CMP), Cardiovascular Multiplex Panel, and Immune Multiplex Panel.\n",
    "\n",
    "Each dataset is organized in long format, where each row corresponds to a single measurement for a given astronaut at a given timepoint.\n",
    "Key columns include:\n",
    "\n",
    "- **analyte** – the biomarker name (e.g., *GLUCOSE, CREATININE, SODIUM, ALBUMIN*).\n",
    "- **value** – measured biomarker value.\n",
    "- **range_min**, **range_max** – clinical reference ranges for each analyte. Depending on the analyte this is the minimum or maximum detectable or it referred to healthy reference ranges.\n",
    "- **units** – measurement units (mg/dL, U/L, etc.).\n",
    "- **test_type** – indicates panel (CMP, cardiovascular multiplex, immune multiplex).\n",
    "- **subject_id** – astronaut ID (C001–C004).\n",
    "- **sex** – M/F.\n",
    "- **timepoint** – collection time relative to flight (*L-92, L-44, L-3 pre-flight*; *R+1, R+45, R+82 post-flight*).\n",
    "\n",
    "These features allow us to:\n",
    "- Track biomarker trajectories across mission phases.\n",
    "- Compare astronaut values to **baseline (pre-flight average)**.\n",
    "- Identify deviations outside clinical reference ranges.\n",
    "- Visualize individual astronaut responses and group trends.\n",
    "\n",
    "## Approach\n",
    "\n",
    "We will build a pipeline that:\n",
    "\n",
    "1. **Load Data**\n",
    "   Merge metadata and biomarker tables.\n",
    "\n",
    "2. **Clean Data**\n",
    "   Handle missing values, outliers, and consistency issues.\n",
    "\n",
    "3. **Identify Data by Phase**\n",
    "   Group values into pre-flight vs. post-flight to allow comparison.\n",
    "\n",
    "4. **Feature Engineering for Visualization**\n",
    "   - Compute **baseline mean** for each analyte (average of L-92, L-44, L-3).\n",
    "   - Calculate **delta = value – baseline** at each post-flight timepoint.\n",
    "   - Flag analytes with statistically or clinically notable deviations.\n",
    "\n",
    "5. **Visualization**\n",
    "   - Line plots of biomarker trajectories across timepoints.\n",
    "   - Highlight analytes with the largest relative changes.\n",
    "\n",
    "6. **Communication**\n",
    "   Produce visuals and narratives that explain *what changed, when it changed, and why it matters* in accessible language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.930157Z",
     "start_time": "2025-09-17T15:23:23.925416Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-caaa3b69ecda13df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this before any other code cell\n",
    "# This downloads the csv data files into the same directory where you have saved this notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATA_DIR = Path(\"cleaned_data\")\n",
    "\n",
    "# Files (pick the one you want to analyze)\n",
    "CMP_FILE      = DATA_DIR / \"Metabolic_Panel.csv\"\n",
    "CARDIO_FILE   = DATA_DIR / \"Serum_Cardiovascular.csv\"\n",
    "IMMUNE_FILE   = DATA_DIR / \"LSDS-8_Multiplex_serum_immune_EvePanel_TRANSFORMED_all_astronauts.csv\"\n",
    "\n",
    "# ==== LOAD DATA ====\n",
    "def load_data(file):\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "cmp = load_data(CMP_FILE)\n",
    "cmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.938574Z",
     "start_time": "2025-09-17T15:23:23.935432Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81357d8c0527b793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "def run_model(X_train,y_train,X_test,y_test):\n",
    "\n",
    "    lin_model = LinearRegression()\n",
    "    lin_model.fit(X_train,y_train)\n",
    "    y_pred = lin_model.predict(X_test)\n",
    "    test_mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    return test_mse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efcc307a64ed278a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load data\n",
    "Create and run a function `load_data()` to do your data loading and any merging needed.  You can specify the arguments and returns as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.944766Z",
     "start_time": "2025-09-17T15:23:23.942648Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b80b9bb85d4194ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file1, file2):\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    merged = pd.merge(df1, df2, how=\"outer\", on=['dteday','hr'])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "The data files were preprocessed using `preprocess.py`\n",
    "\n",
    "This script automates preprocessing of astronaut health data stored in CSV files. It reads raw transformed data from the `data` directory, extracts astronaut identifiers and timepoints, checks for missing/duplicate values, and outputs cleaned datasets in both combined and astronaut-specific formats.\n",
    "\n",
    "#### Features\n",
    "1. **Reads all CSV files** in the specified input directory (`data` by default).  \n",
    "2. **Splits `Sample Name`** column (e.g., `C001_serum_L-3`) into:  \n",
    "   - `astronautID` (e.g., `C001`)  \n",
    "   - `timepoint` (e.g., `L-3`)  \n",
    "3. **Quality checks:**  \n",
    "   - Reports missing values per column.  \n",
    "   - Reports duplicate rows.  \n",
    "4. **Saves outputs:**  \n",
    "   - `<filename>_all_astronauts.csv` → combined file with astronaut/timepoint columns added.  \n",
    "   - `<filename>_<astronautID>.csv` → per-astronaut subsets for easier analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Expected Input Format\n",
    "- CSV files must contain a column named **`Sample Name`**.  \n",
    "- Expected naming convention in `Sample Name`:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values\n",
    "\n",
    "During preprocessing of the metabolic panel data some columns contained missing values.  \n",
    "The following imputation strategy is applied in analysis:\n",
    "\n",
    "| Column                                                                 | Missing Count | Strategy          | Rationale                                   |\n",
    "|------------------------------------------------------------------------|----------------|-------------------|---------------------------------------------|\n",
    "| `total_bilirubin_value_milligram_per_deciliter`                        | 1              | Median imputation | Bilirubin is skewed, median is more robust. |\n",
    "| `bun_to_creatinine_ratio_value`                                        | 22             | Median imputation | Ratio, skewed distribution.                 |\n",
    "| `bun_to_creatinine_ratio_range_min`                                    | 1              | Median imputation | Safe choice for small gaps.                 |\n",
    "| `bun_to_creatinine_ratio_range_max`                                    | 1              | Median imputation | Safe choice for small gaps.                 |\n",
    "| `egfr_african_american_range_max_milliliter_per_minute_per_1.73m²`     | 28             | Dropped  | All values missing         |\n",
    "| `egfr_non_african_american_range_max_milliliter_per_minute_per_1.73m²` | 28             | Dropped   | All values missing         |\n",
    "\n",
    "#### Why this approach?\n",
    "- **Median** is robust to skewed lab values and avoids distortion by outliers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection in Metabolic Panel\n",
    "\n",
    "We applied **boxplot-based outlier detection** (IQR method) across all astronauts combined.  \n",
    "\n",
    "####  Method\n",
    "- For each numeric biomarker:\n",
    "  - Compute **Q1 (25th percentile)** and **Q3 (75th percentile)**.  \n",
    "  - Calculate **IQR = Q3 − Q1**.  \n",
    "  - Define bounds:\n",
    "    - **Lower Bound = Q1 − 1.5 × IQR**  \n",
    "    - **Upper Bound = Q3 + 1.5 × IQR**  \n",
    "  - Any value outside these bounds is flagged as an **outlier**.\n",
    "\n",
    "#### Visualization\n",
    "- Boxplots are generated for each biomarker.  \n",
    "- Outlier points are **annotated with astronautID and timepoint** for traceability.    \n",
    "- A boxplot is only shown if the column contains at least one outlier (to reduce clutter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66c03a03105ee793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Once we had our `final_data`, we applied the `featureEngineering.py` script to enrich the datasets with two key transformations:\n",
    "\n",
    "1. Numeric flight-day scale\n",
    "   - Raw `timepoint` labels like `L-3` and and `R+1` are mapped onto a stretched numeric axis.\n",
    "   - Launch day (`L0`) is set to day 0.\n",
    "   - Pre-launch days count down into the negatives (e.g., `L-3 → -3`).\n",
    "   - In-flight is artificially stretched to **30 days** (`R+0 → 30`), so short spaceflights appear visibly distinct from recovery.\n",
    "   - Recovery days then continue sequentially (`R+1 → 31`, `R+7 → 37`, etc.).\n",
    "   - This stretching is intentional: while it introduces “fake” days, it ensures plots visually distinguish pre-launch**, in-flight, and post-flight phases.\n",
    "\n",
    "2. Derived features\n",
    "   - We compute the Anion Gap as:\n",
    "     $$\n",
    "     \\text{Anion Gap} = [\\text{Na}^+] - [\\text{Cl}^-] - [\\text{CO}_2]\n",
    "     $$\n",
    "   - This metric is clinically relevant for detecting metabolic acidosis and other imbalances, and is not directly provided in the raw data.\n",
    "   - The function adds `anion_gap_value` along with placeholder `min`/`max` columns so the reference ranges can be defined in `stats.py`.\n",
    "\n",
    "Together, these functions clean redundant columns (like `Sample Name`), provide a consistent timeline across astronauts, and introduce clinically meaningful derived biomarkers. This ensures downstream analysis and visualization are both aligned and interpretably enriched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T21:11:38.060956Z",
     "start_time": "2025-09-29T21:11:38.037768Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c4638c5ecf84143",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.featureEngineering import add_flight_day, add_derived_features\n",
    "from scripts.stats import tidy_from_wide\n",
    "\n",
    "# Load cleaned metabolic panel\n",
    "df = pd.read_csv(\"final_data/Metabolic_Panel.csv\")\n",
    "\n",
    "# Feature engineering\n",
    "df = add_flight_day(df)\n",
    "df = add_derived_features(df)\n",
    "\n",
    "# Convert to tidy format\n",
    "tidy_df = tidy_from_wide(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f15ec71cf3a1a85a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Identify All the Statistical Deviance\n",
    "\n",
    "We want a simple table of analytes where an individual astronaut’s R+1 value was statistically higher than baseline (p < 0.05 and R+1 > mean baseline).\n",
    "\n",
    "The code below takes the results from `analyze_r1_vs_L()` and filters for these conditions, then saves the table to CSV for easy inspection.\n",
    "\n",
    "---\n",
    "\n",
    "#### Statistical Background\n",
    "\n",
    "For each astronaut $(a)$ and analyte $(x)$:\n",
    "\n",
    "1. Baseline distribution\n",
    "   Collect all pre-flight measurements (timepoints \\(L-92, L-44, L-3\\)):\n",
    "\n",
    "   $$\n",
    "   L_{a,x} = \\{ v_{a,x}^{L-92}, v_{a,x}^{L-44}, v_{a,x}^{L-3} \\}\n",
    "   $$\n",
    "\n",
    "   with sample mean\n",
    "\n",
    "   $$\n",
    "   \\bar{L}_{a,x} = \\frac{1}{n}\\sum_{i=1}^{n} v_{a,x}^{L_i}\n",
    "   $$\n",
    "\n",
    "   and sample standard deviation\n",
    "\n",
    "   $$\n",
    "   s_{a,x} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(v_{a,x}^{L_i}-\\bar{L}_{a,x})^2}.\n",
    "   $$\n",
    "\n",
    "2. R+1 observation\n",
    "   Take the recovery-day measurement:\n",
    "\n",
    "   $$\n",
    "   R1_{a,x} = v_{a,x}^{R+1}.\n",
    "   $$\n",
    "\n",
    "3. One-sample t-test\n",
    "   Null hypothesis $(H_0)$: astronaut’s R+1 value is consistent with baseline distribution.\n",
    "\n",
    "   Test statistic:\n",
    "\n",
    "   $$\n",
    "   t = \\frac{R1_{a,x} - \\bar{L}_{a,x}}{s_{a,x}/\\sqrt{n}}\n",
    "   $$\n",
    "\n",
    "   where $n = |L_{a,x}|$.\n",
    "   The p-value is computed from Student’s $t$-distribution with $n-1$ degrees of freedom.\n",
    "\n",
    "4. Effect size (Cohen’s d)\n",
    "   To measure magnitude:\n",
    "\n",
    "   $$\n",
    "   d = \\frac{R1_{a,x} - \\bar{L}_{a,x}}{s_{a,x}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "- A significant p-value $(<0.05)$ means it’s unlikely the observed R+1 value arose from random baseline variation.\n",
    "- A positive Cohen’s d indicates the R+1 measurement is elevated above baseline, with larger values meaning stronger effects.\n",
    "- By filtering for cases where $(R1_{a,x} > \\bar{L}_{a,x})$, we isolate biomarkers that spike upward at R+1, which may signal stress, inflammation, or recovery physiology.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T21:12:46.332045Z",
     "start_time": "2025-09-29T21:12:46.215639Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-578054bc7e2183d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scripts.stats import analyze_r1_vs_L, ANALYTE_INFO\n",
    "\n",
    "# Now analyze\n",
    "results_df = analyze_r1_vs_L(tidy_df)\n",
    "\n",
    "elevated = results_df[\n",
    "    (results_df[\"test_type\"] == \"within\") &\n",
    "    (results_df[\"p_value\"] < 0.05) &\n",
    "    (results_df[\"R1\"] > results_df[\"mean_L\"])\n",
    "].copy()\n",
    "\n",
    "elevated[\"label\"] = elevated[\"analyte\"].map(lambda x: ANALYTE_INFO[x][\"label\"])\n",
    "elevated[\"unit\"] = elevated[\"analyte\"].map(lambda x: ANALYTE_INFO[x][\"unit\"])\n",
    "\n",
    "# Reorder for readability\n",
    "elevated = elevated[\n",
    "    [\"astronautID\", \"analyte\", \"label\", \"unit\", \"mean_L\", \"R1\", \"p_value\", \"effect_size\"]\n",
    "]\n",
    "\n",
    "# Save to CSV\n",
    "elevated.to_csv(\"R1_elevated_analytes.csv\", index=False)\n",
    "\n",
    "elevated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create and run the function `build_features()` below to create any additional derivative features (e.g. time series features) that you wish to use in modeling.  You will need to apply this function to both your training and test sets.\n",
    "#### Stats\n",
    "This module provides statistical processing functions\n",
    "for astronaut serum and biochemical datasets.\n",
    "\n",
    "##### Overview\n",
    "--------\n",
    "1. Defines a consistent analyte metadata map (`ANALYTE_INFO`)\n",
    "   that assigns each analyte a human-friendly label and unit.\n",
    "\n",
    "2. Transforms the wide-format astronaut dataset\n",
    "   (value, min, max triplets for each analyte)\n",
    "   into a tidy long-format DataFrame for analysis and plotting.\n",
    "\n",
    "3. Provides statistical comparison functions to test\n",
    "   whether recovery day 1 (R+1) is significantly shifted\n",
    "   relative to the pre-launch baseline (L-series).\n",
    "\n",
    "##### Data Model\n",
    "----------\n",
    "The tidy DataFrame returned by `tidy_from_wide` has the following columns:\n",
    "\n",
    "    astronautID   : string, subject identifier (e.g. \"C001\")\n",
    "    timepoint     : string, raw timepoint (e.g. \"L-3\", \"R+1\")\n",
    "    flight_day    : integer, absolute scale where:\n",
    "                      L-0 = 0 (launch day)\n",
    "                      R+0 = 3 (last day in space)\n",
    "                      R+1 = 4 (first recovery day)\n",
    "                      L-n = -n (pre-launch)\n",
    "    analyte       : string, machine-readable analyte name\n",
    "    value         : numeric, observed measurement\n",
    "    min           : numeric, reference range minimum\n",
    "    max           : numeric, reference range maximum\n",
    "    label         : string, human-readable analyte label\n",
    "    unit          : string, measurement unit\n",
    "\n",
    "##### Statistical Tests\n",
    "-----------------\n",
    "`analyze_r1_vs_L(tidy_df)` applies two complementary analyses:\n",
    "\n",
    "1. Within-astronaut analysis\n",
    "   - For each astronaut and analyte, compares the R+1 measurement\n",
    "     to that astronaut’s baseline distribution of L-values.\n",
    "   - Implemented as a one-sample t-test:\n",
    "         H0: R+1 = mean(L)\n",
    "   - Reports mean baseline, R+1, t-statistic, p-value,\n",
    "     and effect size (Cohen’s d).\n",
    "\n",
    "2. Across-astronaut analysis\n",
    "   - Aggregates astronauts by computing each subject’s baseline mean (L-mean).\n",
    "   - Performs a paired t-test across astronauts:\n",
    "         H0: mean(L-means) = mean(R+1)\n",
    "   - Reports group means, t-statistic, p-value,\n",
    "     and effect size (mean difference / SD of differences).\n",
    "\n",
    "##### Returned Output\n",
    "---------------\n",
    "The results are returned as a DataFrame with columns:\n",
    "\n",
    "    analyte      : analyte name\n",
    "    astronautID  : subject ID (\"ALL\" for group-level test)\n",
    "    test_type    : \"within\" (per astronaut) or \"group\" (across astronauts)\n",
    "    n_L          : number of L timepoints used\n",
    "    mean_L       : baseline mean\n",
    "    R1           : R+1 value (or mean across astronauts)\n",
    "    t_stat       : test statistic\n",
    "    p_value      : two-sided p-value\n",
    "    effect_size  : Cohen’s d\n",
    "\n",
    "##### Use Cases\n",
    "---------\n",
    "- Identify analytes that change significantly at R+1 relative to baseline.\n",
    "- Assess subject-specific vs. group-level recovery patterns.\n",
    "- Provide both inferential results (p-values) and effect sizes\n",
    "  for scientific reporting and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T21:36:55.340995Z",
     "start_time": "2025-09-28T21:36:55.338591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c341995cddfacb2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Feature Selection\n",
    "Use the cell below to create and run the function `feature_select()` which performs feature selection using univariate (filter) methods.  After you analyze the correlations, determine whether you would like to remove any features and do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.966329Z",
     "start_time": "2025-09-17T15:23:23.965199Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d3a200b6fa58a74",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-09ef0b02826676d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Prepare Features for Modeling\n",
    "Our final step in the pipeline is to prepare our feature set for modeling.  In particular, in this step we need to ensure that any categorical variables we may be using are encoded as numeric values in order for the model to function properly.  You might also consider scaling some of your data.\n",
    "\n",
    "In the below cell create and run a function `prepare_train_feats()` which prepares the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.969548Z",
     "start_time": "2025-09-17T15:23:23.968433Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff64eead1e1344c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-051c3772cf091103",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also need to prepare the features in our test set in the same way to feed into the model.  Use the cell below for the function `prepare_test_feats()` which prepares your test set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.972636Z",
     "start_time": "2025-09-17T15:23:23.971570Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb2f6842a35928a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd9d8e5013253357",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run pipeline\n",
    "Finally, let's bring everything together in a function to run the entire pipeline for our training data.  Complete the function `run_pipeline()` in the cell below.  The function should call any/all of the functions you have defined above which are needed to load the data, transform it and prepare the features for both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.976757Z",
     "start_time": "2025-09-17T15:23:23.974943Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f914f1eb4902eff1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def run_pipeline(bike_filename, weather_filename):\n",
    "    '''\n",
    "    Runs your pipeline (calling the above functions as needed) to transform the raw data into the training and test data sets for modeling\n",
    "\n",
    "    Inputs:\n",
    "        bike_filename(str): name of the file containing the bike data\n",
    "        weather_filename(str): name of the file containing the weather data\n",
    "\n",
    "    Returns:\n",
    "        X_train(pd.DataFrame): dataframe containing the training set inputs\n",
    "        y_train(pd.DataFrame): dataframe containing the training set labels\n",
    "        X_test(pd.DataFrame): dataframe containing the test set inputs\n",
    "        y_test(pd.DataFrame): dataframe containing the test set labels\n",
    "    '''\n",
    "    merged = load_data(bike_filename,weather_filename)\n",
    "    clean = clean_data(merged)\n",
    "    X_train, y_train, X_test, y_test = split_data(clean)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-278cf4c4ab73ce46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we've prepared our features we are ready to run our model.  Run the cell below, which trains the model on the training set and calculates and reports the mean squared error (MSE) on the test set.  If everything went well you should have a MSE below 18500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:24.020714Z",
     "start_time": "2025-09-17T15:23:23.981255Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d84dda8b73e82cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bike_datafile = \"2011-2012_bikes.csv\"\n",
    "weather_datafile = \"2011-2012_weather_messy.csv\"\n",
    "X_train, y_train, X_test, y_test = run_pipeline(bike_datafile, weather_datafile)\n",
    "mse_score = run_model(X_train, y_train, X_test, y_test)\n",
    "print('Mean Squared Error on the test set: {:.2f}'.format(mse_score))\n",
    "\n",
    "assert mse_score < 18500"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python XAI (myenv)",
   "language": "python",
   "name": "xaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
