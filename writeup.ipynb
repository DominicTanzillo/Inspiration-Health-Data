{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22d0f94f6aa32bda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7f4291b2367dcbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Visualization of Inspiration Astronaut Biomarkers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72ed6de038e2c607",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Background\n",
    "We are acting as the first data scientist on the Inspiration-Health project. Our first major task is to build a **reproducible pipeline** to clean, transform, and visualize astronaut biomarker data from the SpaceX Inspiration4 mission.\n",
    "Instead of building predictive models, our focus is on **identifying and visualizing significant deviations from baseline values** to help communicate health changes during and after spaceflight.\n",
    "\n",
    "\n",
    "## Data\n",
    "We are working with astronaut biospecimen data from the **SpaceX Inspiration4 Mission** (NASA OSD-575).\n",
    "The cleaned files in `cleaned_data/` include the Comprehensive Metabolic Panel (CMP), Cardiovascular Multiplex Panel, and Immune Multiplex Panel.\n",
    "\n",
    "Each dataset is organized in **long format**, where each row corresponds to a single measurement for a given astronaut at a given timepoint.\n",
    "Key columns include:\n",
    "\n",
    "- **analyte** – the biomarker name (e.g., *GLUCOSE, CREATININE, SODIUM, ALBUMIN*).\n",
    "- **value** – measured biomarker value.\n",
    "- **range_min**, **range_max** – clinical reference ranges for each analyte.\n",
    "- **units** – measurement units (mg/dL, U/L, etc.).\n",
    "- **test_type** – indicates panel (CMP, cardiovascular multiplex, immune multiplex).\n",
    "- **subject_id** – astronaut ID (C001–C004).\n",
    "- **sex** – M/F.\n",
    "- **timepoint** – collection time relative to flight (*L-92, L-44, L-3 pre-flight*; *R+1, R+45, R+82 post-flight*).\n",
    "\n",
    "These features allow us to:\n",
    "- Track biomarker trajectories across mission phases.\n",
    "- Compare astronaut values to **baseline (pre-flight average)**.\n",
    "- Identify deviations outside clinical reference ranges.\n",
    "- Visualize individual astronaut responses and group trends.\n",
    "\n",
    "\n",
    "## Approach\n",
    "\n",
    "We will build a pipeline that:\n",
    "\n",
    "1. **Load Data**\n",
    "   Merge metadata and biomarker tables.\n",
    "\n",
    "2. **Clean Data**\n",
    "   Handle missing values, outliers, and consistency issues.\n",
    "\n",
    "3. **Split Data by Phase**\n",
    "   Group values into pre-flight vs. post-flight to allow comparison.\n",
    "\n",
    "4. **Feature Engineering for Visualization**\n",
    "   - Compute **baseline mean** for each analyte (average of L-92, L-44, L-3).\n",
    "   - Calculate **delta = value – baseline** at each post-flight timepoint.\n",
    "   - Flag analytes with statistically or clinically notable deviations.\n",
    "\n",
    "5. **Visualization**\n",
    "   - Line plots of biomarker trajectories across timepoints.\n",
    "   - Heatmaps of deviations (Δ) per astronaut and analyte.\n",
    "   - Highlight analytes with the largest relative changes.\n",
    "\n",
    "6. **Communication**\n",
    "   Produce visuals and narratives that explain *what changed, when it changed, and why it matters* in accessible language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.930157Z",
     "start_time": "2025-09-17T15:23:23.925416Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-caaa3b69ecda13df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this before any other code cell\n",
    "# This downloads the csv data files into the same directory where you have saved this notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATA_DIR = Path(\"cleaned_data\")\n",
    "\n",
    "# Files (pick the one you want to analyze)\n",
    "CMP_FILE      = DATA_DIR / \"Metabolic_Panel.csv\"\n",
    "CARDIO_FILE   = DATA_DIR / \"LSDS-8_Multiplex_serum_cardiovascular_EvePanel_TRANSFORMED_all_astronauts.csv\"\n",
    "IMMUNE_FILE   = DATA_DIR / \"LSDS-8_Multiplex_serum_immune_EvePanel_TRANSFORMED_all_astronauts.csv\"\n",
    "\n",
    "# ==== LOAD DATA ====\n",
    "def load_data(file):\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "cmp = load_data(CMP_FILE)\n",
    "cmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.938574Z",
     "start_time": "2025-09-17T15:23:23.935432Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81357d8c0527b793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "def run_model(X_train,y_train,X_test,y_test):\n",
    "\n",
    "    lin_model = LinearRegression()\n",
    "    lin_model.fit(X_train,y_train)\n",
    "    y_pred = lin_model.predict(X_test)\n",
    "    test_mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    return test_mse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efcc307a64ed278a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load data\n",
    "Create and run a function `load_data()` to do your data loading and any merging needed.  You can specify the arguments and returns as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.944766Z",
     "start_time": "2025-09-17T15:23:23.942648Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b80b9bb85d4194ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file1, file2):\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    merged = pd.merge(df1, df2, how=\"outer\", on=['dteday','hr'])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66c03a03105ee793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Clean data\n",
    "Use the cell below to create and run a function `clean_data()` which cleans up the data as needed.\n",
    "\n",
    "We started with the Metabolic Pannel which has these rows which we transformed to columns:\n",
    "* Sample Name\n",
    "* albumin_value_gram_per_deciliter\n",
    "* albumin_range_min_gram_per_deciliter\n",
    "* albumin_range_max_gram_per_deciliter\n",
    "* albumin_to_globulin_ratio_value\n",
    "* albumin_to_globulin_ratio_range_min\n",
    "* albumin_to_globulin_ratio_range_max\n",
    "* alkaline_phosphatase_value_units_per_liter\n",
    "* alkaline_phosphatase_range_min_units_per_liter\n",
    "* alkaline_phosphatase_range_max_units_per_liter\n",
    "* alt_value_units_per_liter\n",
    "* alt_range_min_units_per_liter\n",
    "* alt_max_units_per_liter\n",
    "* ast_value_units_per_liter\n",
    "* ast_range_min_units_per_liter\n",
    "* ast_max_units_per_liter\n",
    "* total_bilirubin_value_milligram_per_deciliter\n",
    "* total_bilirubin_range_min_milligram_per_deciliter\n",
    "* total_bilirubin_range_max_milligram_per_deciliter\n",
    "* bun_to_creatinine_ratio_value\n",
    "* bun_to_creatinine_ratio_range_min\n",
    "* bun_to_creatinine_ratio_range_max\n",
    "* calcium_value_milligram_per_deciliter\n",
    "* calcium_range_min_milligram_per_deciliter\n",
    "* calcium_range_max_milligram_per_deciliter\n",
    "* carbon_dioxide_value_millimol_per_liter\n",
    "* carbon_dioxide_range_min_millimol_per_liter\n",
    "* carbon_dioxide_range_max_millimol_per_liter\n",
    "* chloride_value_millimol_per_liter\n",
    "* chloride_range_min_millimol_per_liter\n",
    "* chloride_range_max_millimol_per_liter\n",
    "* creatinine_value_milligram_per_deciliter\n",
    "* creatinine_range_min_milligram_per_deciliter\n",
    "* creatinine_range_max_milligram_per_deciliter\n",
    "* egfr_african_american_value_milliliter_per_minute_per_1.73_meter_squared\n",
    "* egfr_african_american_range_min_milliliter_per_minute_per_1.73_meter_squared\n",
    "* egfr_african_american_range_max_milliliter_per_minute_per_1.73_meter_squared\n",
    "* egfr_non_african_american_value_milliliter_per_minute_per_1.73_meter_squared\n",
    "* egfr_non_african_american_range_min_milliliter_per_minute_per_1.73_meter_squared\n",
    "* egfr_non_african_american_range_max_milliliter_per_minute_per_1.73_meter_squared\n",
    "* globulin_value_gram_per_deciliter\n",
    "* globulin_range_min_gram_per_deciliter\n",
    "* globulin_range_max_gram_per_deciliter\n",
    "* glucose_value_milligram_per_deciliter\n",
    "* glucose_range_min_milligram_per_deciliter\n",
    "* glucose_range_max_milligram_per_deciliter\n",
    "* potassium_value_millimol_per_liter\n",
    "* potassium_range_min_millimol_per_liter\n",
    "* potassium_range_max_millimol_per_liter\n",
    "* total_protein_value_gram_per_deciliter\n",
    "* total_protein_range_min_gram_per_deciliter\n",
    "* total_protein_range_max_gram_per_deciliter\n",
    "* sodium_value_millimol_per_liter\n",
    "* sodium_range_min_millimol_per_liter\n",
    "* sodium_range_max_millimol_per_liter\n",
    "* urea_nitrogen_bun_value_milligram_per_deciliter\n",
    "* urea_nitrogen_bun_range_min_milligram_per_deciliter\n",
    "* urea_nitrogen_bun_range_max_milligram_per_deciliter\n",
    "* astronautID\n",
    "* timepoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T21:05:20.162621Z",
     "start_time": "2025-09-28T21:05:20.158628Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c4638c5ecf84143",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(s1):\n",
    "    # Standardize dates\n",
    "    s1['dteday'] = pd.to_datetime(s1['dteday'], errors='coerce')\n",
    "\n",
    "    # Convert to epoch seconds\n",
    "    s1['dteday'] = (s1['dteday'] + pd.to_timedelta(s1['hr'], unit='h')).astype('int64') // 10**9\n",
    "\n",
    "    # Forcing Data into numeric if not already\n",
    "    for col in s1.columns[1:]:\n",
    "        s1[col] = pd.to_numeric(s1[col], errors='coerce')\n",
    "\n",
    "    # Dropping non-numerics\n",
    "    s2 = s1.dropna()\n",
    "\n",
    "    print(\"Dropped \" + str(len(s1)-len(s2)) + \" rows from our dataset due to missing values.\")\n",
    "\n",
    "    # Remove extrema\n",
    "    numeric_cols = s2.iloc[:, 2:]\n",
    "    s3 = s2.copy()\n",
    "\n",
    "    for col in numeric_cols.columns:\n",
    "        # Get the Series\n",
    "        series = s2[col]\n",
    "\n",
    "        # Calculate mean and std\n",
    "        col_mn = series.mean()\n",
    "        col_st = series.std()\n",
    "\n",
    "        # Mask rows within 2 std\n",
    "        keep_col = (series > col_mn - 2 * col_st) & (series < col_mn + 2 * col_st)\n",
    "\n",
    "        # Drop rows outside this range\n",
    "        s3 = s3[keep_col]\n",
    "    print(\"Dropped \" +str(len(s2)-len(s3))+ \" columns due to extrema\")\n",
    "\n",
    "    s4 = s3.drop(columns=['hr'])\n",
    "\n",
    "    return s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing values\n",
    "\n",
    "In the metabolic panel, there were multiple features with missing values. There were two columns which were completely empty, so we ended up dropping them. For the rest, we went ahead with imputing with mean, median or mode based on the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f15ec71cf3a1a85a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Split data for training and testing\n",
    "Create and run the function `split_data()` in the cell below to split the data into training and test sets.  You should use all data up to and including July 31 2012 as the training set, and the data for the period August 1 2012 - December 31 2012 as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.958281Z",
     "start_time": "2025-09-17T15:23:23.956168Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-578054bc7e2183d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def split_data(s1):\n",
    "    # Define cutoff\n",
    "    cutoff = int(pd.to_datetime(\"2012-08-01 00:00:00\").timestamp())\n",
    "\n",
    "    before = s1[s1['dteday'] <= cutoff]\n",
    "    after  = s1[s1['dteday'] > cutoff]\n",
    "\n",
    "    target = 'cnt'\n",
    "\n",
    "    # Split into X and y\n",
    "    X_train = before.drop(columns=[target])\n",
    "    y_train = before[target]\n",
    "\n",
    "    X_test = after.drop(columns=[target])\n",
    "    y_test = after[target]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create and run the function `build_features()` below to create any additional derivative features (e.g. time series features) that you wish to use in modeling.  You will need to apply this function to both your training and test sets.\n",
    "#### Stats\n",
    "This module provides statistical processing functions\n",
    "for astronaut serum and biochemical datasets.\n",
    "\n",
    "##### Overview\n",
    "--------\n",
    "1. Defines a consistent analyte metadata map (`ANALYTE_INFO`)\n",
    "   that assigns each analyte a human-friendly label and unit.\n",
    "\n",
    "2. Transforms the wide-format astronaut dataset\n",
    "   (value, min, max triplets for each analyte)\n",
    "   into a tidy long-format DataFrame for analysis and plotting.\n",
    "\n",
    "3. Provides statistical comparison functions to test\n",
    "   whether recovery day 1 (R+1) is significantly shifted\n",
    "   relative to the pre-launch baseline (L-series).\n",
    "\n",
    "##### Data Model\n",
    "----------\n",
    "The tidy DataFrame returned by `tidy_from_wide` has the following columns:\n",
    "\n",
    "    astronautID   : string, subject identifier (e.g. \"C001\")\n",
    "    timepoint     : string, raw timepoint (e.g. \"L-3\", \"R+1\")\n",
    "    flight_day    : integer, absolute scale where:\n",
    "                      L-0 = 0 (launch day)\n",
    "                      R+0 = 3 (last day in space)\n",
    "                      R+1 = 4 (first recovery day)\n",
    "                      L-n = -n (pre-launch)\n",
    "    analyte       : string, machine-readable analyte name\n",
    "    value         : numeric, observed measurement\n",
    "    min           : numeric, reference range minimum\n",
    "    max           : numeric, reference range maximum\n",
    "    label         : string, human-readable analyte label\n",
    "    unit          : string, measurement unit\n",
    "\n",
    "##### Statistical Tests\n",
    "-----------------\n",
    "`analyze_r1_vs_L(tidy_df)` applies two complementary analyses:\n",
    "\n",
    "1. **Within-astronaut analysis**\n",
    "   - For each astronaut and analyte, compares the R+1 measurement\n",
    "     to that astronaut’s baseline distribution of L-values.\n",
    "   - Implemented as a one-sample t-test:\n",
    "         H0: R+1 = mean(L)\n",
    "   - Reports mean baseline, R+1, t-statistic, p-value,\n",
    "     and effect size (Cohen’s d).\n",
    "\n",
    "2. **Across-astronaut analysis**\n",
    "   - Aggregates astronauts by computing each subject’s baseline mean (L-mean).\n",
    "   - Performs a paired t-test across astronauts:\n",
    "         H0: mean(L-means) = mean(R+1)\n",
    "   - Reports group means, t-statistic, p-value,\n",
    "     and effect size (mean difference / SD of differences).\n",
    "\n",
    "##### Returned Output\n",
    "---------------\n",
    "The results are returned as a DataFrame with columns:\n",
    "\n",
    "    analyte      : analyte name\n",
    "    astronautID  : subject ID (\"ALL\" for group-level test)\n",
    "    test_type    : \"within\" (per astronaut) or \"group\" (across astronauts)\n",
    "    n_L          : number of L timepoints used\n",
    "    mean_L       : baseline mean\n",
    "    R1           : R+1 value (or mean across astronauts)\n",
    "    t_stat       : test statistic\n",
    "    p_value      : two-sided p-value\n",
    "    effect_size  : Cohen’s d\n",
    "\n",
    "##### Use Cases\n",
    "---------\n",
    "- Identify analytes that change significantly at R+1 relative to baseline.\n",
    "- Assess subject-specific vs. group-level recovery patterns.\n",
    "- Provide both inferential results (p-values) and effect sizes\n",
    "  for scientific reporting and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T21:36:55.340995Z",
     "start_time": "2025-09-28T21:36:55.338591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c341995cddfacb2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Feature Selection\n",
    "Use the cell below to create and run the function `feature_select()` which performs feature selection using univariate (filter) methods.  After you analyze the correlations, determine whether you would like to remove any features and do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.966329Z",
     "start_time": "2025-09-17T15:23:23.965199Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d3a200b6fa58a74",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-09ef0b02826676d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Prepare Features for Modeling\n",
    "Our final step in the pipeline is to prepare our feature set for modeling.  In particular, in this step we need to ensure that any categorical variables we may be using are encoded as numeric values in order for the model to function properly.  You might also consider scaling some of your data.\n",
    "\n",
    "In the below cell create and run a function `prepare_train_feats()` which prepares the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.969548Z",
     "start_time": "2025-09-17T15:23:23.968433Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff64eead1e1344c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-051c3772cf091103",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also need to prepare the features in our test set in the same way to feed into the model.  Use the cell below for the function `prepare_test_feats()` which prepares your test set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.972636Z",
     "start_time": "2025-09-17T15:23:23.971570Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb2f6842a35928a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd9d8e5013253357",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run pipeline\n",
    "Finally, let's bring everything together in a function to run the entire pipeline for our training data.  Complete the function `run_pipeline()` in the cell below.  The function should call any/all of the functions you have defined above which are needed to load the data, transform it and prepare the features for both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.976757Z",
     "start_time": "2025-09-17T15:23:23.974943Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f914f1eb4902eff1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def run_pipeline(bike_filename, weather_filename):\n",
    "    '''\n",
    "    Runs your pipeline (calling the above functions as needed) to transform the raw data into the training and test data sets for modeling\n",
    "\n",
    "    Inputs:\n",
    "        bike_filename(str): name of the file containing the bike data\n",
    "        weather_filename(str): name of the file containing the weather data\n",
    "\n",
    "    Returns:\n",
    "        X_train(pd.DataFrame): dataframe containing the training set inputs\n",
    "        y_train(pd.DataFrame): dataframe containing the training set labels\n",
    "        X_test(pd.DataFrame): dataframe containing the test set inputs\n",
    "        y_test(pd.DataFrame): dataframe containing the test set labels\n",
    "    '''\n",
    "    merged = load_data(bike_filename,weather_filename)\n",
    "    clean = clean_data(merged)\n",
    "    X_train, y_train, X_test, y_test = split_data(clean)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-278cf4c4ab73ce46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we've prepared our features we are ready to run our model.  Run the cell below, which trains the model on the training set and calculates and reports the mean squared error (MSE) on the test set.  If everything went well you should have a MSE below 18500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:24.020714Z",
     "start_time": "2025-09-17T15:23:23.981255Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d84dda8b73e82cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 58 rows from our dataset due to missing values.\n",
      "Dropped 3416 columns due to extrema\n",
      "Mean Squared Error on the test set: 17888.35\n"
     ]
    }
   ],
   "source": [
    "bike_datafile = \"2011-2012_bikes.csv\"\n",
    "weather_datafile = \"2011-2012_weather_messy.csv\"\n",
    "X_train, y_train, X_test, y_test = run_pipeline(bike_datafile, weather_datafile)\n",
    "mse_score = run_model(X_train, y_train, X_test, y_test)\n",
    "print('Mean Squared Error on the test set: {:.2f}'.format(mse_score))\n",
    "\n",
    "assert mse_score < 18500"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "aipi520",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
